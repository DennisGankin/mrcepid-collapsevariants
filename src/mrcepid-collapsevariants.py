#!/usr/bin/env python
# mrcepid-collapsevariants 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import sys
import dxpy
import tarfile
import pandas as pd

from pathlib import Path

from general_utilities.association_resources import generate_linked_dx_file

from general_utilities.job_management.thread_utility import ThreadUtility
from general_utilities.mrc_logger import MRCLogger

# We have to do this to get modules to run properly on DNANexus while still enabling easy editing in PyCharm
sys.path.append('/')
sys.path.append('/collapsevariants/')
sys.path.append('/collapsevariants/tool_parsers/')

# DO NOT move this. It MUST come after the above 'sys.path.append' code to make sure packages run properly
from collapsevariants.ingest_data import IngestData
from collapsevariants.snp_list_generator import SNPListGenerator
from collapsevariants.snp_list_merger import SNPMerger
from collapsevariants.filtering import Filter
from collapsevariants.tool_parsers.bolt_parser import BOLTParser
from collapsevariants.tool_parsers.saige_parser import SAIGEParser
from collapsevariants.tool_parsers.staar_parser import STAARParser, STAARMergingException
from collapsevariants.collapse_logger import CollapseLOGGER

# Set up the system logger – this is NOT the same as LOG_FILE below that records info about the filtering itself
LOGGER = MRCLogger().get_logger()


# Helper thread for running separate BGENs through the collapsing process (functions as a future)
def filter_bgen(chromosome: str, file_prefix: str, chrom_bgen_index: dict) -> tuple:

    # Ingest the filtered BGEN file into this instance
    LOGGER.info(f'Processing bgen: {chromosome}.filtered.bgen')
    bgenprefix = f'filtered_bgen/chr{chromosome}.filtered'  # Get a prefix name for all files

    # Download the requisite files for this chromosome according to the index dict:
    bgen_index = dxpy.DXFile(chrom_bgen_index['index'])
    bgen_sample = dxpy.DXFile(chrom_bgen_index['sample'])
    bgen = dxpy.DXFile(chrom_bgen_index['bgen'])
    dxpy.download_dxfile(bgen_index.get_id(), f'{bgenprefix}.bgen.bgi')
    dxpy.download_dxfile(bgen_sample.get_id(), f'{bgenprefix}.sample')
    dxpy.download_dxfile(bgen.get_id(), f'{bgenprefix}.bgen')

    # Run filtering according to the user-provided filtering expression
    # Also gets the total number of variants retained for this chromosome
    initial_filter = Filter(bgenprefix, chromosome, file_prefix)
    num_variants = initial_filter.num_variants
    LOGGER.info(f'Identified {num_variants} variants that match the given filtering expression in file '
                f'{file_prefix}.{chromosome}.bgen')

    # If there are no variants we need to create an empty dummy file to signify that this process completed BUT
    # no variants were found
    if num_variants == 0:
        LOGGER.warning(f'No files found for chromosome {chromosome}, excluding from final files...')
        return num_variants, chromosome, pd.DataFrame()
    else:
        # Here we are then taking the file generated by run_filtering() and generating various text/plink/vcf files
        # that will be used as part of mrcepid-mergecollapsevariants to generated a merged set of variants we want to
        # test across all VCF files
        # JUST TO BE CLEAR – the names of the functions here are not THAT important. e.g. files generated in the
        # function parse_filters_BOLT() will be used for other tools. It was just for me (Eugene Gardner) to keep things
        # organised when writing this code
        saige_parser = SAIGEParser(file_prefix, chromosome)
        genes, snp_gene_map = saige_parser.genes, saige_parser.snp_gene_map

        bolt_parser = BOLTParser(file_prefix, chromosome, genes, snp_gene_map)
        sample_table = bolt_parser.sample_table

        # STAAR fails sometimes for unknown reasons, so try it twice if it fails before throwing the entire process
        try:
            STAARParser(file_prefix, chromosome)
        except STAARMergingException:
            LOGGER.warning(f'STAAR chr {chromosome} failed to merge, trying again...')
            STAARParser(file_prefix, chromosome)

        # Purge files that we no longer need:
        Path(f'{file_prefix}.{chromosome}.bgen').unlink()
        Path(f'{file_prefix}.{chromosome}.bgen.bgi').unlink()
        Path(f'{file_prefix}.{chromosome}.parsed.txt').unlink()
        Path(f'{file_prefix}.{chromosome}.snps').unlink()

        LOGGER.info(f'Finished bgen: chr{chromosome}.filtered.bgen')
        return num_variants, chromosome, sample_table


@dxpy.entry_point('main')
def main(filtering_expression, snplist, genelist, file_prefix, bgen_index):

    # Set up our logfile for recording information on
    LOG_FILE = CollapseLOGGER(file_prefix)

    # This loads all data
    ingested_data = IngestData(filtering_expression, bgen_index, snplist, genelist)

    # First generate a list of ALL variants genome-wide that we want to retain:
    snp_list_generator = SNPListGenerator(ingested_data.bgen_index,
                                          ingested_data.filtering_expression,
                                          ingested_data.found_snps,
                                          ingested_data.found_genes,
                                          LOG_FILE)

    # Now build a thread worker that contains as many threads
    # instance takes a thread and 1 thread for monitoring
    thread_utility = ThreadUtility()

    # Now loop through each chromosome and do the actual filtering...
    # ...launch the requested threads
    for chromosome in snp_list_generator.chromosomes:
        chrom_bgen_index = ingested_data.bgen_index[chromosome]
        thread_utility.launch_job(filter_bgen,
                                  chromosome=chromosome,
                                  file_prefix=file_prefix,
                                  chrom_bgen_index=chrom_bgen_index)

    # And gather the resulting futures
    per_chromosome_total_sites = 0
    sample_tables = []
    LOG_FILE.write_header('Per-chromosome totals')
    for result in thread_utility:
        per_chromosome_total, chromosome, sample_table = result
        sample_tables.append(sample_table)
        per_chromosome_total_sites += per_chromosome_total
        LOG_FILE.write_int(f'Total sites on chr{chromosome}', per_chromosome_total)
    LOG_FILE.write_spacer()

    # Write a whole bunch of stats
    # Should probably be a separate class, but was tired of refactoring this entire code base...
    LOG_FILE.write_header('Genome-wide totals')
    LOG_FILE.write_int('Total sites expected from filtering expression', snp_list_generator.total_sites)
    LOG_FILE.write_int('Total sites extracted from all chromosomes', per_chromosome_total_sites)
    LOG_FILE.write_int('Total expected and total extracted match',
                       (snp_list_generator.total_sites == per_chromosome_total_sites))
    LOG_FILE.write_spacer()

    # Concatenate and sum sample tables:
    master_sample_table = pd.concat(sample_tables).groupby(['sample_id']).sum().reset_index()
    LOG_FILE.write_header('Per-individual totals')
    LOG_FILE.write_int('Median number of alleles per indv', master_sample_table['ac'].median())
    LOG_FILE.write_int('Median number of genes affected per indv', master_sample_table['ac_gene'].median())
    LOG_FILE.write_float('Mean number of alleles per indv', master_sample_table['ac'].mean())
    LOG_FILE.write_float('Mean number of genes affected per indv', master_sample_table['ac_gene'].mean())
    LOG_FILE.write_int('Max number of alleles', master_sample_table['ac'].max())
    LOG_FILE.write_int('Number of individuals with at least 1 allele',
                       pd.value_counts(master_sample_table['ac'] > 0)[True])
    LOG_FILE.write_spacer()

    LOG_FILE.write_header('AC Histogram')
    LOG_FILE.write_generic('AC_bin\tcount\n')
    ac_counts = master_sample_table.value_counts('ac')
    ac_counts = ac_counts.sort_index()
    for ac, count in ac_counts.iteritems():
        LOG_FILE.write_histogram(ac, count)

    # Here we check if we made a SNP-list. If so, we need to merge across all chromosomes into single per-snp files:
    if ingested_data.found_snps or ingested_data.found_genes:  # run for gene list as well, add
        LOGGER.info('Making merged SNP files for burden testing...')
        SNPMerger(snp_list_generator.chromosomes, file_prefix, ingested_data.found_genes)

    # Because of how I manage the SNP version of this app, I have to delete the sample files here to ensure they are
    # not included in the final tarball:
    for chrom in snp_list_generator.chromosomes:
        Path(f'{file_prefix}.{chrom}.sample').unlink()

    LOGGER.info('Closing LOG file...')
    linked_log_file = LOG_FILE.close_writer()

    # Here we are taking all of the files generated by the various functions above and adding them to a single tar
    # to enable easy output. The only output of this applet is thus a single .tar.gz file per VCF file
    LOGGER.info('Generating final tarball...')
    output_tarball = Path(f'{file_prefix}.tar.gz')
    tar = tarfile.open(output_tarball, 'w:gz')
    for file in Path('./').glob(f'{file_prefix}.*'):
        if '.tar.gz' not in file.name:  # Don't want to remove the tar itself... yet...
            tar.add(file)
            file.unlink()
    tar.close()

    # Set output
    output = {'output_tarball': dxpy.dxlink(generate_linked_dx_file(output_tarball)),
              'log_file': dxpy.dxlink(linked_log_file)}

    return output


dxpy.run()
